#!/usr/bin/env python3
"""
PDFé¢˜ç›®æå–Pipeline (è¶…çº§ä¼˜åŒ–ç‰ˆ)
æ··åˆä½¿ç”¨æ–‡æœ¬æå–å’ŒLLMå¤„ç†ï¼Œå¤§å¹…æå‡æ•ˆç‡
- ä¼˜å…ˆä½¿ç”¨PDFæ–‡æœ¬ç›´æ¥æå–
- å¯¹æ— æ³•æå–æˆ–è´¨é‡ä¸ä½³çš„é¡µé¢ä½¿ç”¨LLM
- ç‰¹åˆ«ä¼˜åŒ–æ ·ä¾‹æ•°æ®çš„æå–ç²¾åº¦
"""

import os
import base64
import json
import time
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import argparse

try:
    import pdf2image
    from PIL import Image
    import requests
    from pdf2image import convert_from_path
    import pdfplumber
    import PyPDF2
except ImportError as e:
    print(f"è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install pdf2image pillow requests pdfplumber PyPDF2")
    print(f"è¿˜éœ€è¦å®‰è£…poppler: ")
    print(f"  Ubuntu/Debian: sudo apt-get install poppler-utils")
    print(f"  macOS: brew install poppler")
    print(f"  Windows: ä¸‹è½½poppleräºŒè¿›åˆ¶æ–‡ä»¶")
    exit(1)

class OptimizedPDFExtractionPipeline:
    def __init__(self, api_key: str, api_base: str = "https://api.openai.com/v1", model: str = "gpt-4-vision-preview", use_cache: bool = True):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆPDFæå–pipeline
        
        Args:
            api_key: LLM APIå¯†é’¥
            api_base: APIåŸºç¡€URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.api_key = api_key
        self.api_base = api_base
        self.model = model
        self.use_cache = use_cache
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ç¼“å­˜å’Œé¡µé¢ç›®å½•
        self.cache_dir = "cache"
        self.pages_dir = "pages"
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.pages_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "text_extracted_pages": 0,
            "llm_processed_pages": 0,
            "cached_pages": 0,
            "total_pages": 0
        }

    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, str]]:
        """
        å°è¯•ç›´æ¥ä»PDFæå–æ–‡æœ¬
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Dict]: æ¯é¡µçš„æ–‡æœ¬å†…å®¹å’Œè´¨é‡è¯„ä¼°
        """
        pages_data = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    
                    if page_text:
                        # è¯„ä¼°æ–‡æœ¬è´¨é‡
                        quality_score = self.evaluate_text_quality(page_text)
                        
                        pages_data.append({
                            "page_num": i + 1,
                            "text": page_text,
                            "quality": quality_score,
                            "extraction_method": "direct_text"
                        })
                    else:
                        pages_data.append({
                            "page_num": i + 1,
                            "text": "",
                            "quality": 0.0,
                            "extraction_method": "needs_llm"
                        })
                        
        except Exception as e:
            print(f"PDFæ–‡æœ¬æå–å¤±è´¥: {e}")
            return []
            
        return pages_data

    def evaluate_text_quality(self, text: str) -> float:
        """
        è¯„ä¼°æå–æ–‡æœ¬çš„è´¨é‡
        
        Args:
            text: æå–çš„æ–‡æœ¬
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0-1)
        """
        if not text or len(text.strip()) < 20:
            return 0.0
        
        quality_indicators = 0
        total_checks = 6
        
        # æ£€æŸ¥1: åŒ…å«é—®é¢˜æ ‡è¯†
        if re.search(r'Problem\s+[A-Z]\.?', text, re.IGNORECASE):
            quality_indicators += 1
            
        # æ£€æŸ¥2: åŒ…å«è¾“å…¥è¾“å‡ºæ ¼å¼
        if re.search(r'(Input|Output|input|output)', text):
            quality_indicators += 1
            
        # æ£€æŸ¥3: åŒ…å«æ—¶é—´/å†…å­˜é™åˆ¶
        if re.search(r'(Time limit|Memory limit|time|memory)', text, re.IGNORECASE):
            quality_indicators += 1
            
        # æ£€æŸ¥4: åŒ…å«æ ·ä¾‹
        if re.search(r'(Example|Sample|standard|input|output)', text, re.IGNORECASE):
            quality_indicators += 1
            
        # æ£€æŸ¥5: æ–‡æœ¬é•¿åº¦åˆç†
        if 100 <= len(text) <= 5000:
            quality_indicators += 1
            
        # æ£€æŸ¥6: æ²¡æœ‰å¤ªå¤šä¹±ç 
        non_ascii_ratio = len([c for c in text if ord(c) > 127]) / len(text)
        if non_ascii_ratio < 0.3:  # å…è®¸30%çš„éASCIIå­—ç¬¦ï¼ˆä¸­æ–‡ç­‰ï¼‰
            quality_indicators += 1
        
        return quality_indicators / total_checks

    def convert_text_to_markdown(self, text: str, page_num: int) -> str:
        """
        å°†æå–çš„æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†markdownæ ¼å¼
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            page_num: é¡µç 
            
        Returns:
            str: æ ¼å¼åŒ–çš„markdownå†…å®¹
        """
        # åŸºç¡€æ¸…ç†
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                cleaned_lines.append(line)
        
        if not cleaned_lines:
            return ""
        
        markdown_content = []
        
        # å¤„ç†æ¯ä¸€è¡Œ
        i = 0
        while i < len(cleaned_lines):
            line = cleaned_lines[i]
            
            # è¯†åˆ«é—®é¢˜æ ‡é¢˜
            if re.match(r'^Problem\s+[A-Z]\.?\s*', line, re.IGNORECASE):
                # æå–é—®é¢˜ç¼–å·å’Œæ ‡é¢˜
                match = re.match(r'^Problem\s+([A-Z])\.?\s*(.*)', line, re.IGNORECASE)
                if match:
                    problem_id = match.group(1)
                    problem_title = match.group(2).strip()
                    if not problem_title and i + 1 < len(cleaned_lines):
                        # æ ‡é¢˜å¯èƒ½åœ¨ä¸‹ä¸€è¡Œ
                        problem_title = cleaned_lines[i + 1].strip()
                        i += 1
                    
                    markdown_content.append(f"## Problem {problem_id}. {problem_title}")
                else:
                    markdown_content.append(f"## {line}")
            
            # è¯†åˆ«æ–‡ä»¶è¾“å…¥è¾“å‡º
            elif 'Input file:' in line or 'Output file:' in line:
                markdown_content.append(f"**{line}**")
            
            # è¯†åˆ«é™åˆ¶æ¡ä»¶
            elif 'Time limit:' in line or 'Memory limit:' in line:
                markdown_content.append(f"**{line}**")
            
            # è¯†åˆ«æ ·ä¾‹éƒ¨åˆ†
            elif re.match(r'^(Example|Sample)', line, re.IGNORECASE):
                markdown_content.append(f"\n### {line}")
                
                # æŸ¥æ‰¾æ ·ä¾‹æ•°æ®
                sample_lines = []
                j = i + 1
                while j < len(cleaned_lines):
                    next_line = cleaned_lines[j]
                    
                    # å¦‚æœé‡åˆ°Noteæˆ–ä¸‹ä¸€ä¸ªéƒ¨åˆ†ï¼Œåœæ­¢
                    if re.match(r'^(Note|Problem|Input|Output)', next_line, re.IGNORECASE):
                        break
                    
                    sample_lines.append(next_line)
                    j += 1
                
                # å¤„ç†æ ·ä¾‹æ•°æ®
                if sample_lines:
                    markdown_content.append("\n**Input:**")
                    markdown_content.append("```")
                    
                    # åˆ†ç¦»è¾“å…¥è¾“å‡º
                    input_lines = []
                    output_lines = []
                    
                    # ç®€å•çš„æ ·ä¾‹æ•°æ®åˆ†ç¦»é€»è¾‘
                    current_section = "input"
                    for sample_line in sample_lines:
                        if re.match(r'^(standard\s+input|standard\s+output)', sample_line, re.IGNORECASE):
                            continue
                        
                        # å°è¯•è¯†åˆ«è¾“å‡ºå¼€å§‹ï¼ˆé€šå¸¸æ˜¯é‡å¤çš„æ•°æ®æˆ–ç‰¹å®šæ ¼å¼ï¼‰
                        if current_section == "input":
                            input_lines.append(sample_line)
                            # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœè¿™è¡Œçœ‹èµ·æ¥åƒè¾“å‡ºç»“æœï¼Œåˆ‡æ¢åˆ°è¾“å‡º
                            if len(input_lines) > 1 and self.looks_like_output(sample_line):
                                output_lines.append(input_lines.pop())
                                current_section = "output"
                        else:
                            output_lines.append(sample_line)
                    
                    # è¾“å‡ºè¾“å…¥éƒ¨åˆ†
                    for line in input_lines:
                        markdown_content.append(line)
                    
                    markdown_content.append("```")
                    
                    # è¾“å‡ºè¾“å‡ºéƒ¨åˆ†
                    if output_lines:
                        markdown_content.append("\n**Output:**")
                        markdown_content.append("```")
                        for line in output_lines:
                            markdown_content.append(line)
                        markdown_content.append("```")
                
                i = j - 1
            
            # è¯†åˆ«Noteéƒ¨åˆ†
            elif line.startswith('Note'):
                markdown_content.append(f"\n### {line}")
            
            # æ™®é€šå†…å®¹
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­¦è¡¨è¾¾å¼
                if re.search(r'\b\d+\s*[\+\-\*/]\s*\d+\b|[A-Za-z]\s*=\s*\d+', line):
                    # å¯èƒ½åŒ…å«æ•°å­¦å†…å®¹ï¼Œç”¨$åŒ…å›´æ•°å­—å’Œå˜é‡
                    formatted_line = self.format_math_content(line)
                    markdown_content.append(formatted_line)
                else:
                    markdown_content.append(line)
            
            i += 1
        
        return '\n'.join(markdown_content)

    def looks_like_output(self, line: str) -> bool:
        """åˆ¤æ–­ä¸€è¡Œæ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥åƒè¾“å‡º"""
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        # å¦‚æœåªåŒ…å«æ•°å­—å’Œç©ºæ ¼ï¼Œä¸”æ•°å­—è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯è¾“å‡º
        if re.match(r'^[\d\s\-]+$', line) and len(line.strip().split()) <= 5:
            return True
        return False

    def format_math_content(self, text: str) -> str:
        """æ ¼å¼åŒ–æ•°å­¦å†…å®¹"""
        # ç®€å•çš„æ•°å­¦æ ¼å¼åŒ–
        # ç”¨$åŒ…å›´å•ç‹¬çš„å˜é‡å’Œæ•°å­—
        formatted = re.sub(r'\b([a-zA-Z])\b', r'$\1$', text)
        formatted = re.sub(r'\b(\d+)\b', r'$\1$', formatted)
        return formatted

    def get_image_hash(self, image_path: str) -> str:
        """è®¡ç®—å›¾ç‰‡çš„SHA256å“ˆå¸Œå€¼"""
        try:
            with open(image_path, 'rb') as f:
                content = f.read()
                return hashlib.sha256(content).hexdigest()
        except Exception as e:
            print(f"è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return ""

    def load_from_cache(self, image_path: str) -> Optional[str]:
        """ä»ç¼“å­˜åŠ è½½LLMå¤„ç†ç»“æœ"""
        if not self.use_cache:
            return None
        
        hash_value = self.get_image_hash(image_path)
        if not hash_value:
            return None
        
        cache_file = os.path.join(self.cache_dir, f"{hash_value}.json")
        
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    return cache_data.get('content')
        except Exception as e:
            print(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        return None

    def save_to_cache(self, image_path: str, content: str):
        """ä¿å­˜LLMå¤„ç†ç»“æœåˆ°ç¼“å­˜"""
        if not self.use_cache:
            return
        
        hash_value = self.get_image_hash(image_path)
        if not hash_value:
            return
        
        cache_file = os.path.join(self.cache_dir, f"{hash_value}.json")
        
        try:
            cache_data = {
                'content': content,
                'timestamp': time.time(),
                'image_path': image_path
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def encode_image_to_base64(self, image_path: str) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
            return ""

    def extract_content_from_image(self, image_path: str, page_num: int, total_pages: int) -> Optional[str]:
        """ä½¿ç”¨LLMä»å›¾ç‰‡ä¸­æå–å†…å®¹"""
        print(f"æ­£åœ¨ç”¨LLMå¤„ç†å›¾ç‰‡: {image_path} (ç¬¬{page_num}/{total_pages}é¡µ)")
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
        cached_content = self.load_from_cache(image_path)
        if cached_content:
            print(f"ç¬¬{page_num}é¡µLLMå†…å®¹ä»ç¼“å­˜åŠ è½½å®Œæˆ ({len(cached_content)} å­—ç¬¦)")
            self.stats["cached_pages"] += 1
            return cached_content
        
        # ç¼–ç å›¾ç‰‡  
        base64_image = self.encode_image_to_base64(image_path)
        
        # é’ˆå¯¹ç®—æ³•ç«èµ›é¢˜ç›®çš„ä¼˜åŒ–æç¤ºè¯
        competition_prompt = f"""è¯·ä»”ç»†åˆ†æè¿™å¼ ç®—æ³•ç«èµ›é¢˜ç›®å›¾ç‰‡ï¼Œç²¾ç¡®æå–æ‰€æœ‰å†…å®¹ã€‚è¿™æ˜¯ç¬¬{page_num}é¡µ(å…±{total_pages}é¡µ)ã€‚

**é‡è¦æç¤ºï¼šè¯·å®Œæ•´æå–é¡µé¢ä¸Šçš„æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œç‰¹åˆ«æ³¨æ„æ ·ä¾‹æ•°æ®çš„æ ¼å¼ï¼**

**è¯†åˆ«è§„åˆ™:**
1. **æ–°é¢˜ç›®è¯†åˆ«**: 
   - ä»…å½“é¡µé¢ä¸Šæœ‰æ˜æ˜¾çš„é¢˜ç›®å¼€å§‹æ ‡è®°(å¦‚"Problem X")å¹¶ä¸”æœ‰é¢˜ç›®æè¿°æ—¶ï¼Œæ‰ä½¿ç”¨ ## Problem X. é¢˜ç›®åç§° æ ¼å¼

2. **æ ·ä¾‹æ ¼å¼(æå…¶å…³é”®)**:
   - æ ·ä¾‹æ•°æ®å¿…é¡»å®Œå…¨æŒ‰åŸå§‹æ ¼å¼è¾“å‡ºï¼Œä¿æŒæ‰€æœ‰ç©ºæ ¼ã€æ¢è¡Œå’Œå¯¹é½
   - ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
   ```
   å…·ä½“çš„è¾“å…¥/è¾“å‡ºæ•°æ®ï¼ˆå®Œå…¨æŒ‰åŸæ ·ï¼Œä¸è¦æ”¹å˜ä»»ä½•å­—ç¬¦ï¼‰
   ```

3. **æ•°å­¦å…¬å¼**: ä½¿ç”¨LaTeXæ ¼å¼ $formula$

4. **å®Œæ•´æå–**: ä¸è¦é—æ¼ä»»ä½•å†…å®¹ï¼ŒåŒ…æ‹¬é™åˆ¶æ¡ä»¶ã€æ³¨é‡Šç­‰

è¯·ç›´æ¥è¾“å‡ºmarkdownæ ¼å¼å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""
        
        # æ„å»ºè¯·æ±‚
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": competition_prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 8000
        }
        
        # å‘é€è¯·æ±‚
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f"{self.api_base}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"ç¬¬{page_num}é¡µLLMå¤„ç†å®Œæˆ ({len(content)} å­—ç¬¦)")
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    self.save_to_cache(image_path, content)
                    self.stats["llm_processed_pages"] += 1
                    
                    return content
                else:
                    print(f"LLMè¯·æ±‚å¤±è´¥: {response.status_code}")
                    if attempt < max_retries - 1:
                        time.sleep(5)
                    
            except Exception as e:
                print(f"LLMå¤„ç†å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
        
        return None

    def convert_pdf_to_images(self, pdf_path: str, dpi: int = 300) -> List[str]:
        """å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡"""
        try:
            print(f"å¼€å§‹è½¬æ¢PDFä¸ºå›¾ç‰‡ (DPI: {dpi})")
            
            images = convert_from_path(pdf_path, dpi=dpi)
            image_paths = []
            
            for i, image in enumerate(images):
                image_path = os.path.join(self.pages_dir, f"page_{i+1:03d}.png")
                image.save(image_path, 'PNG')
                image_paths.append(image_path)
                print(f"ç¬¬ {i+1} é¡µè½¬æ¢å®Œæˆ: {image_path}")
            
            print(f"PDFè½¬æ¢å®Œæˆï¼Œå…± {len(image_paths)} é¡µ")
            return image_paths
            
        except Exception as e:
            print(f"PDFè½¬æ¢å¤±è´¥: {e}")
            return []

    def process_pdf_optimized(self, pdf_path: str, output_file: str, debug: bool = False) -> bool:
        """
        ä¼˜åŒ–ç‰ˆPDFå¤„ç†ä¸»å‡½æ•°
        æ··åˆä½¿ç”¨æ–‡æœ¬æå–å’ŒLLMå¤„ç†
        """
        print(f"ğŸš€ å¼€å§‹ä¼˜åŒ–å¤„ç†PDF: {pdf_path}")
        print("=" * 60)
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {k: 0 for k in self.stats}
        
        # æ­¥éª¤1: å°è¯•ç›´æ¥æå–æ–‡æœ¬
        print("ğŸ“ æ­¥éª¤1: å°è¯•ç›´æ¥æå–PDFæ–‡æœ¬...")
        pages_text_data = self.extract_text_from_pdf(pdf_path)
        
        if not pages_text_data:
            print("âŒ æ–‡æœ¬æå–å¤±è´¥ï¼Œå›é€€åˆ°çº¯LLMæ¨¡å¼")
            return self.fallback_to_llm_only(pdf_path, output_file, debug)
        
        self.stats["total_pages"] = len(pages_text_data)
        
        # æ­¥éª¤2: å†³å®šæ¯é¡µçš„å¤„ç†æ–¹å¼
        print("ğŸ§  æ­¥éª¤2: åˆ†ææ¯é¡µæœ€ä½³å¤„ç†æ–¹å¼...")
        processing_plan = []
        
        for page_data in pages_text_data:
            page_num = page_data["page_num"]
            quality = page_data["quality"]
            
            if quality >= 0.6:  # æ–‡æœ¬è´¨é‡è¶³å¤Ÿå¥½
                processing_plan.append({
                    "page_num": page_num,
                    "method": "text_only",
                    "quality": quality
                })
                print(f"  ç¬¬{page_num}é¡µ: æ–‡æœ¬æå– (è´¨é‡: {quality:.2f})")
            else:
                processing_plan.append({
                    "page_num": page_num, 
                    "method": "llm_required",
                    "quality": quality
                })
                print(f"  ç¬¬{page_num}é¡µ: éœ€è¦LLMå¤„ç† (è´¨é‡: {quality:.2f})")
        
        # æ­¥éª¤3: æ‰§è¡Œå¤„ç†è®¡åˆ’
        print("âš™ï¸  æ­¥éª¤3: æ‰§è¡Œæ··åˆå¤„ç†...")
        
        all_contents = []
        image_paths = []
        
        # åªä¸ºéœ€è¦LLMå¤„ç†çš„é¡µé¢ç”Ÿæˆå›¾ç‰‡
        llm_pages = [p for p in processing_plan if p["method"] == "llm_required"]
        if llm_pages:
            print(f"ğŸ“¸ ä¸º{len(llm_pages)}é¡µç”Ÿæˆå›¾ç‰‡...")
            image_paths = self.convert_pdf_to_images(pdf_path)
        
        # å¤„ç†æ¯é¡µ
        for i, plan in enumerate(processing_plan):
            page_num = plan["page_num"]
            method = plan["method"]
            
            if method == "text_only":
                # ç›´æ¥ä½¿ç”¨æå–çš„æ–‡æœ¬
                page_data = pages_text_data[i]
                markdown_content = self.convert_text_to_markdown(page_data["text"], page_num)
                all_contents.append(markdown_content)
                self.stats["text_extracted_pages"] += 1
                print(f"âœ… ç¬¬{page_num}é¡µæ–‡æœ¬å¤„ç†å®Œæˆ ({len(markdown_content)} å­—ç¬¦)")
                
            else:
                # ä½¿ç”¨LLMå¤„ç†
                if i < len(image_paths):
                    image_path = image_paths[i]
                    llm_content = self.extract_content_from_image(image_path, page_num, len(processing_plan))
                    if llm_content:
                        all_contents.append(llm_content)
                    else:
                        print(f"âš ï¸  ç¬¬{page_num}é¡µLLMå¤„ç†å¤±è´¥")
                        all_contents.append("")
        
        # æ­¥éª¤4: åˆå¹¶å†…å®¹
        print("ğŸ”— æ­¥éª¤4: åˆå¹¶é¢˜ç›®å†…å®¹...")
        final_content = self.merge_problem_content(all_contents, debug)
        
        # æ­¥éª¤5: ä¿å­˜ç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_content)
            print(f"âœ… å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_file}")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  æ€»é¡µæ•°: {self.stats['total_pages']}")
            print(f"  æ–‡æœ¬æå–: {self.stats['text_extracted_pages']} é¡µ")
            print(f"  LLMå¤„ç†: {self.stats['llm_processed_pages']} é¡µ")
            print(f"  ç¼“å­˜å‘½ä¸­: {self.stats['cached_pages']} é¡µ")
            
            efficiency = (self.stats['text_extracted_pages'] + self.stats['cached_pages']) / self.stats['total_pages'] * 100
            print(f"  å¤„ç†æ•ˆç‡: {efficiency:.1f}% (æ— éœ€æ–°LLMè°ƒç”¨)")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def fallback_to_llm_only(self, pdf_path: str, output_file: str, debug: bool = False) -> bool:
        """å›é€€åˆ°çº¯LLMæ¨¡å¼"""
        print("ğŸ”„ å›é€€åˆ°çº¯LLMå¤„ç†æ¨¡å¼...")
        
        # å¯¼å…¥åŸæœ‰çš„å¤„ç†ç®¡é“
        from pdf_extraction_pipeline import PDFExtractionPipeline
        
        old_pipeline = PDFExtractionPipeline(self.api_key, self.api_base, self.model, self.use_cache)
        return old_pipeline.process_pdf(pdf_path, output_file, debug)

    def merge_problem_content(self, contents: List[str], debug: bool = False) -> str:
        """åˆå¹¶é—®é¢˜å†…å®¹"""
        if not contents:
            return ""
        
        # ç®€åŒ–çš„åˆå¹¶é€»è¾‘
        merged_content = []
        current_problem = []
        
        problem_pattern = re.compile(r'^##\s*Problem\s+\w+\.')
        
        for content in contents:
            if not content.strip():
                continue
                
            lines = content.split('\n')
            
            for line in lines:
                if problem_pattern.match(line):
                    # å‘ç°æ–°é—®é¢˜ï¼Œä¿å­˜å½“å‰é—®é¢˜
                    if current_problem:
                        merged_content.append('\n'.join(current_problem))
                        current_problem = []
                    current_problem.append(line)
                else:
                    current_problem.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªé—®é¢˜
        if current_problem:
            merged_content.append('\n'.join(current_problem))
        
        return '\n\n'.join(merged_content)

# æ·»åŠ å‘½ä»¤è¡Œæ¥å£
def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆPDFç®—æ³•ç«èµ›é¢˜ç›®æå–å·¥å…·')
    parser.add_argument('pdf_file', help='è¾“å…¥PDFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='extracted_content.md', help='è¾“å‡ºmarkdownæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-k', '--api-key', required=True, help='LLM APIå¯†é’¥')
    parser.add_argument('-b', '--api-base', default='https://dashscope.aliyuncs.com/compatible-mode/v1', help='APIåŸºç¡€URL')
    parser.add_argument('-m', '--model', default='qwen-vl-max', help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('-d', '--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.pdf_file):
        print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_file}")
        return False
    
    # åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆpipeline
    pipeline = OptimizedPDFExtractionPipeline(
        api_key=args.api_key,
        api_base=args.api_base,
        model=args.model,
        use_cache=not args.no_cache
    )
    
    # å¤„ç†PDF
    success = pipeline.process_pdf_optimized(args.pdf_file, args.output, args.debug)
    
    if success:
        print("ğŸ‰ å¤„ç†å®Œæˆ!")
        return True
    else:
        print("âŒ å¤„ç†å¤±è´¥!")
        return False

if __name__ == "__main__":
    main()
