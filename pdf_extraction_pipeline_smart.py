#!/usr/bin/env python3
"""
PDFé¢˜ç›®æå–Pipeline (æ™ºèƒ½ä¼˜åŒ–ç‰ˆ)
ä¸»è¦ä½¿ç”¨LLMå¤„ç†ï¼Œä½†åœ¨æ ·ä¾‹éƒ¨åˆ†æä¾›åŸå§‹æ–‡æœ¬å‚è€ƒ
- ä¿æŒLLMå¤„ç†ä»¥ç¡®ä¿LaTeXæ ¼å¼æ­£ç¡®
- ä¸ºæ ·ä¾‹éƒ¨åˆ†æå–åŸå§‹æ–‡æœ¬ä½œä¸ºå‚è€ƒ
- è®©LLMåŸºäºåŸå§‹æ ·ä¾‹æ–‡æœ¬è¿›è¡Œç²¾ç¡®æ ¼å¼åŒ–
"""

import os
import base64
import json
import time
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import argparse

try:
    import pdf2image
    from PIL import Image
    import requests
    from pdf2image import convert_from_path
    import pdfplumber
except ImportError as e:
    print(f"è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install pdf2image pillow requests pdfplumber")
    print(f"è¿˜éœ€è¦å®‰è£…poppler: ")
    print(f"  Ubuntu/Debian: sudo apt-get install poppler-utils")
    print(f"  macOS: brew install poppler")
    print(f"  Windows: ä¸‹è½½poppleräºŒè¿›åˆ¶æ–‡ä»¶")
    exit(1)

class SmartPDFExtractionPipeline:
    def __init__(self, api_key: str, api_base: str = "https://api.openai.com/v1", model: str = "gpt-4-vision-preview", use_cache: bool = True):
        """
        åˆå§‹åŒ–æ™ºèƒ½PDFæå–pipeline
        
        Args:
            api_key: LLM APIå¯†é’¥
            api_base: APIåŸºç¡€URL
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.api_key = api_key
        self.api_base = api_base
        self.model = model
        self.use_cache = use_cache
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ç¼“å­˜å’Œé¡µé¢ç›®å½•
        self.cache_dir = "cache"
        self.pages_dir = "pages"
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.pages_dir, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "enhanced_with_text": 0,
            "pure_llm": 0,
            "cached_pages": 0,
            "total_pages": 0
        }

    def extract_sample_text_from_pdf(self, pdf_path: str) -> Dict[int, str]:
        """
        ä»PDFä¸­æå–æ ·ä¾‹ç›¸å…³çš„åŸå§‹æ–‡æœ¬
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[int, str]: é¡µç  -> è¯¥é¡µçš„æ ·ä¾‹æ–‡æœ¬
        """
        sample_texts = {}
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    
                    if page_text and self.contains_sample_data(page_text):
                        # æå–æ ·ä¾‹ç›¸å…³çš„æ–‡æœ¬æ®µè½
                        sample_text = self.extract_sample_section(page_text)
                        if sample_text:
                            sample_texts[i + 1] = sample_text
                            print(f"ç¬¬{i+1}é¡µæ£€æµ‹åˆ°æ ·ä¾‹æ•°æ®ï¼Œæå–äº†{len(sample_text)}å­—ç¬¦çš„å‚è€ƒæ–‡æœ¬")
                            
        except Exception as e:
            print(f"æå–æ ·ä¾‹æ–‡æœ¬å¤±è´¥: {e}")
            
        return sample_texts

    def contains_sample_data(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«æ ·ä¾‹æ•°æ®"""
        sample_indicators = [
            'Example',
            'Sample',
            'standard input',
            'standard output',
            'Input:',
            'Output:'
        ]
        
        text_lower = text.lower()
        for indicator in sample_indicators:
            if indicator.lower() in text_lower:
                return True
        return False

    def extract_sample_section(self, text: str) -> str:
        """æå–æ ·ä¾‹ç›¸å…³çš„æ–‡æœ¬æ®µè½"""
        lines = text.split('\n')
        sample_lines = []
        in_sample_section = False
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹æ ·ä¾‹æ®µè½å¼€å§‹
            if re.search(r'(Example|Sample|standard\s+input|Input:|Output:)', line, re.IGNORECASE):
                in_sample_section = True
                sample_lines.append(line)
                continue
            
            # æ£€æµ‹æ ·ä¾‹æ®µè½ç»“æŸ
            if in_sample_section:
                # å¦‚æœé‡åˆ°æ–°çš„é¢˜ç›®æˆ–ç« èŠ‚ï¼Œç»“æŸæ ·ä¾‹æå–
                if re.search(r'(Problem\s+[A-Z]|Note|Input\s*$|Output\s*$|^[A-Z][a-z]+:)', line):
                    if not line.lower().startswith(('input', 'output')):
                        break
                
                sample_lines.append(line)
        
        return '\n'.join(sample_lines)

    def get_image_hash(self, image_path: str) -> str:
        """è®¡ç®—å›¾ç‰‡çš„SHA256å“ˆå¸Œå€¼"""
        try:
            with open(image_path, 'rb') as f:
                content = f.read()
                return hashlib.sha256(content).hexdigest()
        except Exception as e:
            print(f"è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return ""

    def load_from_cache(self, image_path: str) -> Optional[str]:
        """ä»ç¼“å­˜åŠ è½½LLMå¤„ç†ç»“æœ"""
        if not self.use_cache:
            return None
        
        hash_value = self.get_image_hash(image_path)
        if not hash_value:
            return None
        
        cache_file = os.path.join(self.cache_dir, f"{hash_value}.json")
        
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    return cache_data.get('content')
        except Exception as e:
            print(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        return None

    def save_to_cache(self, image_path: str, content: str):
        """ä¿å­˜LLMå¤„ç†ç»“æœåˆ°ç¼“å­˜"""
        if not self.use_cache:
            return
        
        hash_value = self.get_image_hash(image_path)
        if not hash_value:
            return
        
        cache_file = os.path.join(self.cache_dir, f"{hash_value}.json")
        
        try:
            cache_data = {
                'content': content,
                'timestamp': time.time(),
                'image_path': image_path
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def encode_image_to_base64(self, image_path: str) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
            return ""

    def extract_content_from_image_enhanced(self, image_path: str, page_num: int, total_pages: int, sample_reference: Optional[str] = None) -> Optional[str]:
        """
        ä½¿ç”¨LLMä»å›¾ç‰‡ä¸­æå–å†…å®¹ï¼Œå¯é€‰æ‹©æä¾›æ ·ä¾‹å‚è€ƒæ–‡æœ¬
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            page_num: é¡µç 
            total_pages: æ€»é¡µæ•°
            sample_reference: å¯é€‰çš„æ ·ä¾‹å‚è€ƒæ–‡æœ¬
        """
        print(f"æ­£åœ¨ç”¨LLMå¤„ç†å›¾ç‰‡: {image_path} (ç¬¬{page_num}/{total_pages}é¡µ)")
        if sample_reference:
            print(f"  >> æä¾›äº†{len(sample_reference)}å­—ç¬¦çš„æ ·ä¾‹å‚è€ƒæ–‡æœ¬")
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
        cache_key = f"{image_path}_{hashlib.md5((sample_reference or '').encode()).hexdigest()}"
        cached_content = self.load_from_cache(cache_key)
        if cached_content:
            print(f"ç¬¬{page_num}é¡µå†…å®¹ä»ç¼“å­˜åŠ è½½å®Œæˆ")
            self.stats["cached_pages"] += 1
            return cached_content
        
        # ç¼–ç å›¾ç‰‡  
        base64_image = self.encode_image_to_base64(image_path)
        
        # æ„å»ºå¢å¼ºçš„æç¤ºè¯
        if sample_reference:
            competition_prompt = f"""è¯·ä»”ç»†åˆ†æè¿™å¼ ç®—æ³•ç«èµ›é¢˜ç›®å›¾ç‰‡ï¼Œç²¾ç¡®æå–æ‰€æœ‰å†…å®¹ã€‚è¿™æ˜¯ç¬¬{page_num}é¡µ(å…±{total_pages}é¡µ)ã€‚

**é‡è¦æç¤ºï¼šæˆ‘å·²ç»ä¸ºä½ æä¾›äº†æœ¬é¡µçš„æ ·ä¾‹æ•°æ®å‚è€ƒæ–‡æœ¬ï¼Œè¯·ç¡®ä¿æ ·ä¾‹éƒ¨åˆ†å®Œå…¨æŒ‰ç…§å‚è€ƒæ–‡æœ¬çš„æ•°æ®å†…å®¹ï¼**

**æ ·ä¾‹å‚è€ƒæ–‡æœ¬ï¼š**
```
{sample_reference}
```

**å¤„ç†è¦æ±‚ï¼š**
1. **æ–°é¢˜ç›®è¯†åˆ«**: 
   - ä»…å½“é¡µé¢ä¸Šæœ‰æ˜æ˜¾çš„é¢˜ç›®å¼€å§‹æ ‡è®°(å¦‚"Problem X")æ—¶ï¼Œæ‰ä½¿ç”¨ ## Problem X. é¢˜ç›®åç§° æ ¼å¼

2. **æ ·ä¾‹å¤„ç†(æå…¶é‡è¦)**:
   - ä½¿ç”¨ä¸Šé¢æä¾›çš„æ ·ä¾‹å‚è€ƒæ–‡æœ¬ä¸­çš„ç²¾ç¡®æ•°æ®
   - å°†æ ·ä¾‹æ ¼å¼åŒ–ä¸ºæ¸…æ™°çš„Input/Outputç»“æ„
   - æ ·ä¾‹æ•°æ®å¿…é¡»ä¸å‚è€ƒæ–‡æœ¬å®Œå…¨ä¸€è‡´ï¼Œä¸è¦ä¿®æ”¹ä»»ä½•æ•°å­—æˆ–å­—ç¬¦
   - ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
   ```
   **Input:**
   å…·ä½“è¾“å…¥æ•°æ®(ä¸å‚è€ƒæ–‡æœ¬ä¸€è‡´)

   **Output:**
   å…·ä½“è¾“å‡ºæ•°æ®(ä¸å‚è€ƒæ–‡æœ¬ä¸€è‡´)
   ```

3. **æ•°å­¦å…¬å¼**: æ‰€æœ‰æ•°å­¦å†…å®¹ä½¿ç”¨LaTeXæ ¼å¼ï¼Œå¦‚ $n$ã€$10^9$ã€$$formula$$ï¼Œä¸è¦ä½¿ç”¨\(\)

4. **å®Œæ•´æå–**: åŒ…æ‹¬é¢˜ç›®æè¿°ã€é™åˆ¶æ¡ä»¶ã€æ³¨é‡Šç­‰æ‰€æœ‰å†…å®¹

è¯·ç›´æ¥è¾“å‡ºmarkdownæ ¼å¼å†…å®¹ï¼Œç¡®ä¿æ ·ä¾‹æ•°æ®ä¸æä¾›çš„å‚è€ƒæ–‡æœ¬å®Œå…¨ä¸€è‡´ï¼"""
            
            self.stats["enhanced_with_text"] += 1
        else:
            competition_prompt = f"""è¯·ä»”ç»†åˆ†æè¿™å¼ ç®—æ³•ç«èµ›é¢˜ç›®å›¾ç‰‡ï¼Œç²¾ç¡®æå–æ‰€æœ‰å†…å®¹ã€‚è¿™æ˜¯ç¬¬{page_num}é¡µ(å…±{total_pages}é¡µ)ã€‚

**è¯†åˆ«è§„åˆ™:**
1. **æ–°é¢˜ç›®è¯†åˆ«**: 
   - ä»…å½“é¡µé¢ä¸Šæœ‰æ˜æ˜¾çš„é¢˜ç›®å¼€å§‹æ ‡è®°(å¦‚"Problem X")æ—¶ï¼Œæ‰ä½¿ç”¨ ## Problem X. é¢˜ç›®åç§° æ ¼å¼

2. **æ ·ä¾‹æ ¼å¼**:
   - æ ·ä¾‹æ•°æ®å¿…é¡»å®Œå…¨æŒ‰åŸå§‹æ ¼å¼è¾“å‡º
   - ä½¿ç”¨æ¸…æ™°çš„Input/Outputç»“æ„ï¼š
   ```
   **Input:**
   å…·ä½“è¾“å…¥æ•°æ®

   **Output:**  
   å…·ä½“è¾“å‡ºæ•°æ®
   ```

3. **æ•°å­¦å…¬å¼**: ä½¿ç”¨LaTeXæ ¼å¼ $formula$ æˆ– $$formula$$

4. **å®Œæ•´æå–**: ä¸è¦é—æ¼ä»»ä½•å†…å®¹

è¯·ç›´æ¥è¾“å‡ºmarkdownæ ¼å¼å†…å®¹ã€‚"""
            
            self.stats["pure_llm"] += 1
        
        # æ„å»ºè¯·æ±‚
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text",
                            "text": competition_prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 8000
        }
        
        # å‘é€è¯·æ±‚
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f"{self.api_base}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"ç¬¬{page_num}é¡µå¤„ç†å®Œæˆ ({len(content)} å­—ç¬¦)")
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    self.save_to_cache(cache_key, content)
                    
                    return content
                else:
                    print(f"LLMè¯·æ±‚å¤±è´¥: {response.status_code}")
                    if attempt < max_retries - 1:
                        time.sleep(5)
                    
            except Exception as e:
                print(f"LLMå¤„ç†å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
        
        return None

    def convert_pdf_to_images(self, pdf_path: str, dpi: int = 300) -> List[str]:
        """å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡"""
        try:
            print(f"å¼€å§‹è½¬æ¢PDFä¸ºå›¾ç‰‡ (DPI: {dpi})")
            
            images = convert_from_path(pdf_path, dpi=dpi)
            image_paths = []
            
            for i, image in enumerate(images):
                image_path = os.path.join(self.pages_dir, f"page_{i+1:03d}.png")
                image.save(image_path, 'PNG')
                image_paths.append(image_path)
                print(f"ç¬¬ {i+1} é¡µè½¬æ¢å®Œæˆ: {image_path}")
            
            print(f"PDFè½¬æ¢å®Œæˆï¼Œå…± {len(image_paths)} é¡µ")
            return image_paths
            
        except Exception as e:
            print(f"PDFè½¬æ¢å¤±è´¥: {e}")
            return []

    def process_pdf_smart(self, pdf_path: str, output_file: str, debug: bool = False) -> bool:
        """
        æ™ºèƒ½PDFå¤„ç†ä¸»å‡½æ•°
        ä¸»è¦ä½¿ç”¨LLMï¼Œä½†ä¸ºæ ·ä¾‹æä¾›åŸå§‹æ–‡æœ¬å‚è€ƒ
        """
        print(f"ğŸš€ å¼€å§‹æ™ºèƒ½å¤„ç†PDF: {pdf_path}")
        print("ğŸ“ ç­–ç•¥: LLMä¸»å¯¼ + æ ·ä¾‹æ–‡æœ¬å¢å¼º")
        print("=" * 60)
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {k: 0 for k in self.stats}
        
        # æ­¥éª¤1: æå–æ ·ä¾‹å‚è€ƒæ–‡æœ¬
        print("ğŸ” æ­¥éª¤1: æå–æ ·ä¾‹å‚è€ƒæ–‡æœ¬...")
        sample_references = self.extract_sample_text_from_pdf(pdf_path)
        
        if sample_references:
            print(f"âœ… åœ¨{len(sample_references)}é¡µä¸­å‘ç°æ ·ä¾‹æ•°æ®")
            for page_num, ref_text in sample_references.items():
                print(f"  ç¬¬{page_num}é¡µ: {len(ref_text)}å­—ç¬¦çš„æ ·ä¾‹å‚è€ƒ")
        else:
            print("â„¹ï¸ æœªå‘ç°æ˜æ˜¾çš„æ ·ä¾‹æ•°æ®ï¼Œå°†ä½¿ç”¨çº¯LLMæ¨¡å¼")
        
        # æ­¥éª¤2: è½¬æ¢PDFä¸ºå›¾ç‰‡
        print("ğŸ“¸ æ­¥éª¤2: è½¬æ¢PDFä¸ºå›¾ç‰‡...")
        image_paths = self.convert_pdf_to_images(pdf_path)
        
        if not image_paths:
            print("âŒ PDFè½¬æ¢å¤±è´¥")
            return False
        
        self.stats["total_pages"] = len(image_paths)
        
        # æ­¥éª¤3: å¤„ç†æ¯é¡µ
        print("ğŸ§  æ­¥éª¤3: ä½¿ç”¨LLMå¤„ç†æ¯é¡µ...")
        all_contents = []
        
        debug_log_file = None
        if debug:
            debug_log_file = f"{os.path.splitext(pdf_path)[0]}_smart_debug.log"
            with open(debug_log_file, 'w', encoding='utf-8') as log_f:
                log_f.write(f"æ™ºèƒ½PDFå¤„ç†è°ƒè¯•æ—¥å¿—\n")
                log_f.write(f"PDFæ–‡ä»¶: {pdf_path}\n")
                log_f.write(f"å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                log_f.write("=" * 80 + "\n\n")
        
        for i, image_path in enumerate(image_paths):
            page_num = i + 1
            
            # è·å–è¯¥é¡µçš„æ ·ä¾‹å‚è€ƒæ–‡æœ¬
            sample_ref = sample_references.get(page_num)
            
            # å¤„ç†é¡µé¢
            content = self.extract_content_from_image_enhanced(
                image_path, page_num, len(image_paths), sample_ref
            )
            
            if content:
                all_contents.append(content)
                
                # è°ƒè¯•æ—¥å¿—
                if debug and debug_log_file:
                    with open(debug_log_file, 'a', encoding='utf-8') as log_f:
                        log_f.write(f"=== ç¬¬ {page_num} é¡µ ===\n")
                        log_f.write(f"å›¾ç‰‡è·¯å¾„: {image_path}\n")
                        log_f.write(f"å¤„ç†æ–¹å¼: {'æ ·ä¾‹å¢å¼º' if sample_ref else 'çº¯LLM'}\n")
                        log_f.write(f"è¾“å‡ºé•¿åº¦: {len(content)} å­—ç¬¦\n")
                        
                        if sample_ref:
                            log_f.write(f"\n--- æ ·ä¾‹å‚è€ƒæ–‡æœ¬ ---\n")
                            log_f.write(sample_ref)
                            log_f.write(f"\n")
                        
                        log_f.write(f"\n--- LLMè¾“å‡º ---\n")
                        log_f.write(content)
                        log_f.write(f"\n\n{'='*50}\n\n")
                        
            else:
                print(f"âš ï¸ ç¬¬{page_num}é¡µå¤„ç†å¤±è´¥")
                all_contents.append("")
        
        # æ­¥éª¤4: åˆå¹¶å†…å®¹
        print("ğŸ”— æ­¥éª¤4: åˆå¹¶é¢˜ç›®å†…å®¹...")
        final_content = self.merge_problem_content(all_contents, debug)
        
        # æ­¥éª¤5: ä¿å­˜ç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_content)
            print(f"âœ… å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_file}")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  æ€»é¡µæ•°: {self.stats['total_pages']}")
            print(f"  æ ·ä¾‹å¢å¼º: {self.stats['enhanced_with_text']} é¡µ")
            print(f"  çº¯LLMå¤„ç†: {self.stats['pure_llm']} é¡µ")
            print(f"  ç¼“å­˜å‘½ä¸­: {self.stats['cached_pages']} é¡µ")
            
            if debug and debug_log_file:
                print(f"  è°ƒè¯•æ—¥å¿—: {debug_log_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def merge_problem_content(self, contents: List[str], debug: bool = False) -> str:
        """åˆå¹¶é—®é¢˜å†…å®¹"""
        if not contents:
            return ""
        
        # å¯¼å…¥åŸæœ‰çš„åˆå¹¶é€»è¾‘
        from pdf_extraction_pipeline import PDFExtractionPipeline
        
        dummy_pipeline = PDFExtractionPipeline("", "", "")
        return dummy_pipeline.merge_problem_content(contents, debug)

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            import shutil
            if os.path.exists(self.cache_dir):
                shutil.rmtree(self.cache_dir)
                os.makedirs(self.cache_dir, exist_ok=True)
                print("âœ… ç¼“å­˜å·²æ¸…ç©º")
        except Exception as e:
            print(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")

    def get_cache_info(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        count = 0
        total_size = 0
        
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    count += 1
                    file_path = os.path.join(self.cache_dir, filename)
                    total_size += os.path.getsize(file_path)
        except Exception as e:
            print(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            
        return {"count": count, "size": total_size}

# æ·»åŠ å‘½ä»¤è¡Œæ¥å£
def main():
    parser = argparse.ArgumentParser(description='æ™ºèƒ½PDFç®—æ³•ç«èµ›é¢˜ç›®æå–å·¥å…·')
    parser.add_argument('pdf_file', help='è¾“å…¥PDFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='extracted_content.md', help='è¾“å‡ºmarkdownæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-k', '--api-key', required=True, help='LLM APIå¯†é’¥')
    parser.add_argument('-b', '--api-base', default='https://dashscope.aliyuncs.com/compatible-mode/v1', help='APIåŸºç¡€URL')
    parser.add_argument('-m', '--model', default='qwen-vl-max', help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('-d', '--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç©ºç¼“å­˜')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ™ºèƒ½pipeline
    pipeline = SmartPDFExtractionPipeline(
        api_key=args.api_key,
        api_base=args.api_base,
        model=args.model,
        use_cache=not args.no_cache
    )
    
    if args.clear_cache:
        pipeline.clear_cache()
        return True
    
    if not os.path.exists(args.pdf_file):
        print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {args.pdf_file}")
        return False
    
    # å¤„ç†PDF
    success = pipeline.process_pdf_smart(args.pdf_file, args.output, args.debug)
    
    if success:
        print("ğŸ‰ æ™ºèƒ½å¤„ç†å®Œæˆ!")
        return True
    else:
        print("âŒ å¤„ç†å¤±è´¥!")
        return False

if __name__ == "__main__":
    main()
